# Math

* [신뢰 구간의 정의는 무엇인가요?](#7)  

표본 통계량에서 파생되어 알 수없는 모집단 모수 값이 포함될 가능성이 있는 값의 범위입니다.

> 95% 신뢰구간의 의미란 무엇인가?

    같은 모형에서 반복해서 표본을 얻고, 신뢰구간을 얻을 때 신뢰구간이 참 모수값을 포함할 확률이 95%가 되도록 만들어지는 구간.

* [p-value를 모르는 사람에게 설명한다면 어떻게 설명하실 건가요?](#8) 

P-value는 귀무가설이 맞다는 전제하에 통계값이 실제로 관측될 값 이상일 확률을 의미합니다.

> 귀무가설은 무엇?

    모집단의 특성에 대해 옳다고 제안하는 잠정적인 주장입니다.
    귀무가설은 "모집단의 모수는 OO와 같다" 또는 "모집단의 모수는 OO와 차이가 없다" 라고 가정하는 것을 말합니다.

P-value는 1종 오류를 범할 확률을 의미합니다. 예를 들어 P-value가 5% 라면 100번중 5번 1종 오류가 발생한다는 말입니다. 검정을 할때 유의수준 $a$를 정하는데 이것이 1종 오류의 상한선이 됩니다.

# ML

* [LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요?](#7)  

**LSA(Latent Semantic Analysis) 잠재 의미 분석**

다변량 통계분석 방법이며 고차원 데이터 공간에 대해 축을 변경하여 데이터에 내재해 있는 구조를 밝히는 기법입니다. 축을 찾아내기 위해 **SVD** 를 사용합니다.

**LDA(Latent Dirichlet Allocation) 잠재 디리클레 할당**

LDA란 텍스트 마이닝 방법중 하나입니다. 텍스트를 DTM(Document-Term Matrix)로 만들어 단어의 등장빈도에 따라 텍스트를 분류합니다. 단어의 경향성을 파악하고 그 문서를 대표하는 특정 토픽을 찾아내는 것이 LDA의 목적입니다.

**SVD(Singular Value Decomposition) 특이값 분해**

임의의 m*n 차원의 행렬 A에 대하여 다음과 같이 행렬을 분해할 수 있는 행렬 분해의 방법중 하나입니다.

![](https://i.imgur.com/WO5yHQB.png)



* [Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요?](#8)  

마르코프 체인의 정의란 마르코프 성질을 가진 이산 확률과정을 뜻합니다. 여기서 마르코프 성질은 '특정 상태의 확률은 오직 과거의 상태에 의존한다'라는 것입니다. 예를 들어 오늘의 날씨가 맑다면 내일의 날씨는 맑을지 비가 내릴지를 확률적으로 표현할 수 있습니다.

마르코프 체인(Markov Chain)이란 한 상태에서 다른 상태로 이전을 할 때, 특정한 확률적인 특성을 따르는 것을 의미하는데, 마르코프 체인을 대표하는 가장 중요한 성질은 현재 상태에서 다음 상태로 넘어갈 때 현재 시점보다 이전의 과거 상태에는 의존을 하지 않는다는 것입니다. 추상적으로 느껴지는 개념이지만 사실 사례를 통해서 살펴보면 꽤나 간단한 원리이고, 이 특성을 이용하면 복잡한 joint Distribution을 손쉽게 계산할 수 있습니다. 강화 학습을 공부하거나 베이지안 통계학을 공부하면 마르코프 체인이라는 개념이 많이 등장하는데 흔히 쓰이는 MCMC (Markov Chain Monte Carlo)를 이해하는 것에도 필수적인 개념입니다.

# DL

- [하이퍼 파라미터는 무엇인가요?](#7)  

모델의 학습 과정에 반영되며, 학습을 시작하기 전에 미리 사용자가 값을 결정하는 것을 말합니다. 좋은 모델을 만들기 위해서는 학습 알고리즘이나 모델의 구조 등을 이해하고 하이퍼 파라미터를 잘 튜닝해야 합니다.


- [Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?](#8)  

1. 상수 기반 초기화 (Zeros, Ones, Constant)

정해진 숫자를 기반으로 weight 값을 초기화 합니다. 0으로 초기화하면 Zeros, 1로 초기화하면 Ones, 사용자가 지정한 상수로 초기화하면 COnstant 기법입니다.
    
    한계점: 모든 웨이트들이 동일한 값으로 초기화 되어 학습 초기에 모든 웨이트 값들이 동일한 출력을 내게 됩니다. 모든 hidden layer들이 같은, 또는 대칭적인 Weight 값을 갖는다면 모든 노드의 활성화 함수와 같은, 또는 대칭적인 입력값이 들어간다는 뜻이고, 그렇게 되면 역전파 시 모든 가중치의 값이 똑같이 갱신되기 때문에 가중치를 여러 개로 둔 의미가 없어집니다. 즉 한 layer의 노드들이 모두 같은 일을 하기 때문에 노드의 낭비가 발생합니다.

2. 선형 대수 기반 초기화 (Orthogonal, Identity)

선형대수에 등장하는 개념을 기반으로 weight 값을 초기화 합니다. Orthogonal weight 초기화란 직교 행렬이 되게끔 matrix 형태의 weight 값을 초기화 하겠다는 의미입니다. 직교 행렬이란 자기 자신과 자신의 전치 행렬의 곱이 항등행렬이 되는 행렬을 말합니다.

    한계점 : 주어진 조건에만 맞도록 랜덤하게 초기값을 설정하여 특정 값이 지나치게 크거나 작아질 수 있습니다. 이는 레이어들의 출력 값이 들쑥날쑥하게 만들어 학습을 어렵게 만듭니다.


3. 확률 분포 기반 초기화(RandomUniform, RandomNormal, TruncatedNormal)

특정한 확률 분포에 기반하여 랜덤한 값을 추출하여 웨이트를 초기화합니다. 이 때 UniformDistribution과 NormalDistribution이 사용됩니다. 각각의 확률 분포의 그래프는 아래와 같습니다.

![](https://i.imgur.com/FSQafqA.png)


    한계점 : 레이어가 깊어질 수록 전체 신경망에 걸쳐 non-homogeneous distributions of activations 를 생성합니다. activation이 0에 수렴하여 vanishing gradient 문제의 원인이 됩니다.

    - 평균이 0, 표준편차가 1인 정규분포를 따르는 가중치 값들로 초기화했을때 : sigmoid가 활성화 함수로 쓰인 layer들을 통과할수록 그 출력 값이 0, 1에 치우치게 되고, sigmoid의 값이 0,1에 치우친다는 것은 곧 그 지점에서의 gradient도 0에 가까워진다는 것이므로 gradient vanishing 문제가 발생합니다.

    - 평균이 0, 표준편차가 0.01인 정규분포를 따르는 가중치 값들로 초기화 했을때 : sigmoid가 활성화 함수로 쓰인 layer들을 통과할수록 그 출력 값이 0.5에 치우친다. 0,1에 치우치지는 않았으나 활성화 값들이 치우쳤다는 것은 다수의 노드(뉴런)들이 거의 같은 값으로 출력하고 있으니 여러 노드를 둔 의미가 없다는 것 입니다 (표현력 제한 문제)


4. 분산 조정 기반 초기화(Glorot, Lecun, He)

분산 조정 기법이란 확률 분포를 기반으로 추출한 값으로 웨이트를 초기화 하되, 이 확률 분포의 분산을 웨이트 별로 동적으로 조절해주자는 것 입니다. 그리고 분산을 조정할 때에는 해당 weight에 input으로 들어오는 tensor의 차원과 결과 값으로 출력하는 tensor의 차원이 사용됩니다.

가중치가 너무 고르게, 또는 너무 랜덤하게 분포되는 상황은 막되, 활성화 값의 분포는 고르게 되어야 하기 때문에, 각 츠으이 활성화 값들을 광범위하게 분포시킬 목적으로 가중치의 적절한 분포를 찾고자 합니다. 

**해결책**

    Xavier Initialization , He Initialization



[참고자료.1](https://yeomko.tistory.com/40)
[참고자료.2](https://mole-starseeker.tistory.com/m/42?category=859657)