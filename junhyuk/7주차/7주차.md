* 어떨 때 모수적 방법론을 쓸 수 있고, 어떨 때 비모수적 방법론을 쓸 수 있나요?  

모수적 방법이 뭐에요? - 정규성을 갖는다는 모수적 특성을 이용하는 통계적 방법을 모수적 방법이라고 합니다.

비모수적 방법은 뭐가 있을까요? - 자료를 크기 순으로 배열하여 순위를 매긴 다음 순위의 합을 통해 차이를 비교하는 순위합검정같은 경우 모수의 특성을 이용하지 않기 때문에 비모수적 방법이라고 합니다. 숫자로는 표현되지만 수량화할 수 없고, 평균을 낼 수도 없는 순위 척도의 경우에는 비록 연속형 자료는 아니지만, 순위의 합을 이용하는 비모수적 방법을 적용하는 것이 가능합니다.

표본의 개수가 충분히 크거나, 정규성 검정에서 정규분포로 간주되는 연속형 자료의 경우 모수적 방법을 사용할 수 있으며, 그 외의 경우에는 비모수적 방법을 사용합니다.

* “likelihood”와 “probability”의 차이는 무엇일까요?  

확률은 고정된 확률분포에서 어떠한 관측값이 나타나는지에 대한 확률이고, 가능도는 고정된 관측값이 어떠한 확률분포에서 어느정도의 확률로 나타내는지에 대한 확률을 뜻합니다.

연속형 확률변수의 경우, 특정 관측치가 일어날 가능성을 비교하기 어려운데 가능도를 활용하여 확률밀도함수(PDF) 값을 구하여 비교할 수 있습니다.

* Gradient Descent에 대해서 쉽게 설명한다면?  

    Gradient Descent는 파라미터에 대해 오차값을 미분하여 그 기울기값(Gradient)를 구하고 경사가 하강하는 방향으로 파라미터를 업데이트 하는 방법입니다.

  * 왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까?  



  * GD 중에 때때로 Loss가 증가하는 이유는?  

    ![](https://i.imgur.com/L7O0sZ3.png)

    Gradient가 양수인 방향으로도 파라미터 update step을 가져 가는 경우가 생길 수 있으며, 이 경우에 Loss가 일시적으로 증가할 수 있습니다


  * Back Propagation에 대해서 쉽게 설명 한다면?  

  신경망의 최종 단계에서 계산된 오차의 변화량을 바탕으로 이전 단계의 파라미터를 업데이트 하는 방향을 설정하는 방법입니다.


* Association Rule의 Support, Confidence, Lift에 대해 설명해주세요.

Support란 전체 경우의 수에서 두 아이템이 같이 나오는 비율을 의미합니다.

![](https://i.imgur.com/SbCUF5O.png)

Confidence는 X가 나온 경우 중 X와 Y가 함께 나올 비율을 의미합니다.

![](https://i.imgur.com/JX78E2e.png)

Lift는 X와 Y가 같이 나오는 비율을 X가 나올 비율과 Y가 나올 비율의 곱으로 나눈 값 입니다.

![](https://i.imgur.com/8Se9MiC.png)


* 최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요?  

Newton's Method 방법은 현재 x값에서 접선을 그리고 접선이 x축과 만나는 지점으로 x를 이동시켜 가면서 점진적으로 해를 찾는 방법입니다.

![](https://i.imgur.com/KnM3H5f.png)
