* 중심극한정리는 왜 유용한걸까요?  

모집단이 어떤 크기를 가지고 있던 간에 일단 표본의 크기가 충분히 크다면, 표본 평균들의 분포가 모집단의 모수를 기반으로 한 정규분포를 이룬다는 점을 이용하여 특정 사건이 일어날 수 있는 확률 값을 계산할 수 있게 됩니다.

![](https://i.imgur.com/k9bgdtH.png)

* 엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요.  

엔트로피는 확률분포가 가지는 정보의 확신도 혹은 정보량을 수치로 표현한 것입니다. 확률분포에서 값이 나올 확률이 높아지고 나머지 값의 확률은 낮아진다면 엔트로피는 작아집니다. 반대로 여러가지 값이 나올 확률이 대부분 비슷한 경우에는 엔트로피가 높아집니다.

Information Gain(정보획득량)은 X라는 조건에 의해 확률 변수 Y의 엔트로피가 얼마나 감소하였는가를 나타내는 값 입니다.

* 뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가? 

네트워크를 학습할 때, 많은 양의 데이터를 필요로 하는 단점이 있습니다. 이를 위해 나온 one-shot learning은 새로운 클래스에 대해서 데이터 샘플을 한개(one)만 주어 이를 구별하는 것입니다.

* 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?  

sigmoid는 relu보다 exp(x) 연산때문에 cost가 더 소모됩니다(ㄹㅇ?)

|x| > 10 에서 saturated 되기 때문에 gradient가 거의 0이 되어버리는 vanishing gradient 현상이 발생합니다.



  * Non-Linearity라는 말의 의미와 그 필요성은?

  데이터의 차원이 높아지게 되면 데이터의 분포는 선형(Linear) 형태가 아닌 비 선형(Non-Linear)형태를 띄게 되고, 이러한 데이터 분포를 가지는 경우 단순히 1차식의 Linear 형태로는 데이터를 표현할 수 없기 때문에 Non-Linearity가 필요합니다.

  * ReLU로 어떻게 곡선 함수를 근사하나?  

Multi-layer의 activation으로 ReLu를 사용하게 되면 단순히 Linear 한 형태에서 더 나아가 Linear 한 부분 부분의 결합의 합성 함수가 만들어 지게 됩니다. 이 Linear한 결합 구간 구간을 최종적으로 보았을 때, 최종적으로 Non-linearity 한 성질을 가지게 됩니다. 학습 과정에서 BackPropagation 할 때에, 이 모델은 데이터에 적합하도록 fitting이 되게 됩니다. ReLu의 장점인 gradient가 출력층과 멀리 있는 layer 까지 전달된다는 성질로 인하여 데이터에 적합하도록 fitting이 잘되게 되고 이것으로 곡선 함수에 근사하도록 만들어 지게 됩니다.

  * ReLU의 문제점은?  

ReUL의 문제점은 입력값이 0보다 작을 때(음수일 때), 전부 0을 출력하기 때문에 몇몇 Weight들이 업데이트 되지 않는 Dying ReLU현상이 발생하게 됩니다. 

  * Bias는 왜 있는걸까?  

bias-variance trade-off 란 말이 있는데, 가정을 잘못 하게 되었을 때 발생하는 오차를 줄이기 위해 존재합니다.

* 다른 좋은 머신 러닝 대비, 오래된 기법인 나이브 베이즈(naive bayes)의 장점을 옹호해보세요.  

1. 계산복잡성이 낮아 시간 소요가 짧습니다.

2. 노이즈와 결측데이터가 있어도 잘 수행합니다.

3. 훈련에 대한 상대적으로 적은 예제가 필요하지만, 많은 예제에 대해서도 효과가 좋습니다.

4. 예측에 대한 추정된 확률을 얻기 쉽습니다.

* 회귀 / 분류시 알맞은 metric은 무엇일까?  

회귀 - $R^2$ (결정계수, Coefficient of determination)

![](https://i.imgur.com/IMyjdkr.png)

분류 - F1 Score, Log Loss/Binary Coressentropy , categorical Crossentropy

![](https://i.imgur.com/N9cwHV5.png)

Log Loss/Binary Coressentropy

![](https://i.imgur.com/K2wvfUf.png)

categorical Crossentropy

![](https://i.imgur.com/QVu1H15.png)
