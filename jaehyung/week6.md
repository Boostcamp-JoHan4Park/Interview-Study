## Statistics/Math  
* 중심극한정리는 왜 유용한걸까요?  
중심극한정리란 표본추출이 무수히 많이 수행되면(보통 30회 이상), 표본 평균의 분포가 정규분포에 수렴한다는 것  중심극한정리가 유용한 이유는 모집단의 형태에 상관없이 표본 평균의 분포가 정규분포를 따르기 때문이다.

* 엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요.  
entropy는 주어진 데이터의 혼잡도를 의미하고, entropy는 다음과 같이 데이터가 어떤 클래스에 속할 확률에 대한 기댓값으로 표현할 수 있음. information gain은 어떤 속성을 선택함으로 인해 데이터가 잘 필터링되는지 말하고, 1에서 엔트로피를 뺀 값으로 표현됨.  

## Deep Learning  
* 뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가?  
답을 내기 위해서 답에 대한 많은 데이터를 학습해야함. One-shot Learning은 뉴럴넷도 새로운 레이블을 지닌 데이터가 적을 때에도 모델이 좋은 성능을 내도록 사용되는 방법.  
* 요즘 sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?  
기울기 소실 문제의 이유가 있다. ReLU값이 양수일 때, 기울기가 1이므로 연쇄 곲이 1보다 작아지는 것을 어느정도 막아줄 수 있다.  
<br>

## Machine Learning  
* 다른 좋은 머신 러닝 대비, 오래된 기법인 나이브 베이즈(naive bayes)의 장점을 옹호해보세요.  
데이터에서 변수들에 대한 조건부 독립을 가정하는 알고리즘으로 클래스에 대한 사전 정보와 데이터로부터 추출된 정보를 결합하고, 베이즈 정리를 이용하여 어떤 데이터가 특정 클래스에 속하는지 분류하는 알고리즘.  
장점 : 단순하고 빠르며 매우 효과적, 노이즈와 결측 데이터가 있어도 잘 수행함, 훈련에 대한 상대적으로 적은 예제가 필요하지만 매우 많은 예제도 수행함, 예측에 대한 추정된 확률을 얻기 쉬움.  
* 회귀 / 분류시 알맞은 metric은 무엇일까?  
분류의 경우 : 이진분류이면 Binary Crossentropy, 분류해야할 클래스가 3개 이상이면 Categorical Crossentropy  
회귀의 경우 : R^2 결정계수 : 독립변수의 개수가 많아질수록 결정 계수가 1에 가까워진다.  
