* Statistics/Math  
    * 통계에서 사용되는 bootstrap의 의미는 무엇인가요.  
    가설 검증을 하거나 메트릭을 계산하기 전에 random sampling을 직용하는 방법이다. 예를들어 어떤 집단에서 값을 측정했을 때, 그 중에서 임의로 100개를 뽑아서 평균을 구하는 것.  
    ML에서는 랜덤 샘플링을 통해 training data를 늘리는 방법
    * 모수가 매우 적은 (수십개 이하) 케이스의 경우 어떤 방식으로 예측 모델을 수립할 수 있을까요?  
    질문에서 모수가 아니라 모집단의 수로 쓰인것 같음  
    표본이 매우 작으면 표본평균의 분포가 정규분포를 따른다고 가정할 수 없으므로 비모수적 방법을 이용해서 예측 모델을 수립한다.  
    * 베이지안과 프리퀀티스트 간의 입장차이를 설명해주실 수 있나요?  
    베이지안은 사건의 확률을 바라볼 때, 사전확률을 미리 생각하고 사건의 발생에 따라 베이즈 정리로 사후 확률을 구해 다시 사전 확률을 업데이트 함. 과거의 사건이 현재 사건에 영향을 끼친다.  
    프리퀀티스트는 현재의 객관적인 확률에 의해서만 사건이 발생한다는 입장을 가지고 있음.  

* Deep-learning  
    * Training 세트와 Test 세트를 분리하는 이유는?  
    학습한 데이터에 대해서는 성능이 잘 나오지만 한번도 본적 없는 데이터에 대해서도 좋은 결과를 내기 위해서 한번도 본적이 없는 test 세트를 이용해서 성능을 판단한다.  
    * Validation 세트가 따로 있는 이유는?  
    train data로 학습을 시키고 valid를 통해 모델의 성능 평가를 하고 좋은 방향으로 모델을 수정하고 최종적으로 만들어진 모델로 test 데이터로 최종 성능 평가를 한다. train과 test로만 하면 결국 test에 오버피팅 될수 있기 때문.  
    * Test 세트가 오염되었다는 말의 뜻은?  
    test 데이터가 train과 유사하거나 포함된 경우를 말함.  
    * Regularization이란 무엇인가?  
    모델의 오버피팅을 막고 일반화를 하는 것   


* Machine-learning  
    * 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?  
    머신러닝은 모델의 성능을 높이는게 목적  
    통계는....?  
    * 인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요?  
    딥러닝 이전에는 선형적으로만 수행하여 레이어를 깊게 쌓지 못해서 복잡한 문제를 풀 수 없었다.  
    * 지금 나오고 있는 deep learning 계열의 혁신의 근간은 무엇이라고 생각하시나요?  
    GPU의 발전?, 좋은 데이터  ㄴ
